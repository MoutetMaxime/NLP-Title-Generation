{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMIyjj8TskT_"
      },
      "source": [
        "<center>\n",
        "<h1><br\\></h1>\n",
        "<h1>INF582: INTRODUCTION TO TEXT MINING AND NLP</h1>\n",
        "</center>\n",
        "<center>\n",
        "<h2>Lab Session 1: TF-IDF</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "<h3><b>1. Introduction</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Documents are traditionally represented with the vector space model, also known as the bag-of-words representation [<a href='https://nlp.stanford.edu/IR-book/' >Manning et al.</a>]. With this approach, each document di from the collection D = {d1 . . . dm } of size m is associated with an n-dimensional feature vector, where n is the number of unique terms in the preprocessed collection. The set of unique terms T = {t1 . . . tn } is called the vocabulary.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF), is a method used to compute the coordinates of a document in the vector space.\n",
        "</p>\n",
        "\n",
        "<h3><b>2. Learning Objective</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this lab, you will learn how to compute the TF-IDF representation and use it in 3 different tasks:\n",
        "<ul>\n",
        "<li>computing the cosine similarity between documents</li>\n",
        "<li>executing a query against a collection of documents using the inverted index,</li>\n",
        "<li>performing supervised document classification.</li>\n",
        "</ul>\n",
        "</p>\n",
        "\n",
        "<h3><b>3. Computing TF-IDF</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "The assumption is that the importance of a word to a document increases when its frequency increases. However, considering the frequency as the only factor to judge the importance of a word would result in giving greater weight to commonly used terms such as stopwords, which could be misleading in many tasks. TF-IDF mitigates this problem by introducing a factor that diminishes the weight of words that occur frequently in other documents in the same collection. The TF-IDF weight computation is based on the product of two separate factors, namely the Term Frequency (TF) and the Inverse Document Frequency (IDF). More specifically:\n",
        "\n",
        "$$ tf\\text{-}idf(t,d,D) = tf(t,d) \\times idf(t, D) $$\n",
        "\n",
        "There are many ways to determine tf and idf . In our case, $tf (t, d)$ is the number of times term $t$ appears in document $d$, and $idf (t, D) = ln (\\frac{m}{1+df (t)}) + 1$, with $df (t)$ the number of documents in the collection $D$ that contains $t$. We can notice that the weight of a term in a document increases when its frequency increases in this document (first factor), and decreases when the number of documents in the collection containing this term increases (second factor). M ∈ $R^{m×n}$ is the TF-IDF matrix of $D$, where $M_{ij}$ is the TF-IDF weight of the $jth$ word in the $ith$ document.\n",
        "\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU8DZ-kVPuBX"
      },
      "source": [
        "### Importing libraries and downloading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcO2sbrulEJ",
        "outputId": "a726f385-5a7d-49e1-e58e-979c82b52160"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('webkb-test-stemmed.txt', <http.client.HTTPMessage at 0x7dd7a3c170d0>)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152573&authkey=AFz5kPjESCHRl3s\",\n",
        "    \"webkb-train-stemmed.txt\",\n",
        ")\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2152576&authkey=ACypGA77xWokzQ8\",\n",
        "    \"webkb-test-stemmed.txt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kaN3aNU_svtv"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"euler is the father of graph theory\",\n",
        "    \"graph theory studies the properties of graphs\",\n",
        "    \"bioinformatics studies the application of efficient algorithms in the biological field\",\n",
        "]\n",
        "\n",
        "\n",
        "class tfidf(object):\n",
        "    def __init__(self, collection, stop_words=[]):\n",
        "        \"\"\"collection is a list of strings\"\"\"\n",
        "        self.word2ind = {}  # map each word in the collection to a unique index\n",
        "        self.ind2word = {}\n",
        "        self.word2idf = {}  # map each word to its idf\n",
        "        self.collection = collection\n",
        "        self.documents = [document.split() for document in collection]\n",
        "\n",
        "        self.unique_words = set()  # list of unique words in the collection\n",
        "        for sent in self.documents:\n",
        "            for w in sent:\n",
        "                self.unique_words.add(w)\n",
        "        for stp_w in stop_words:\n",
        "            self.unique_words.remove(stp_w)\n",
        "        self.unique_words = list(self.unique_words)\n",
        "\n",
        "        self.count_unique_words = len(self.unique_words)\n",
        "        self.word2ind = dict(zip(self.unique_words, range(self.count_unique_words)))\n",
        "        self.ind2word = {v: k for k, v in self.word2ind.items()}\n",
        "        self.count_documents = len(collection)\n",
        "\n",
        "        # compute the idf of unqiue words in the collection\n",
        "        for word in self.word2ind.keys():\n",
        "            count = 0  # number of documents that contains word\n",
        "            for d in self.documents:\n",
        "                if word in d:\n",
        "                    count += 1\n",
        "\n",
        "            idf = np.log(self.count_documents / (count + 1)) + 1\n",
        "            self.word2idf[word] = idf\n",
        "\n",
        "    def getWordFromInd(self, ind):\n",
        "        return self.ind2word[ind]\n",
        "\n",
        "    def getListWords(self):\n",
        "        return self.unique_words\n",
        "\n",
        "    def tf(self, document):\n",
        "        \"\"\"\n",
        "        return the frequency of each unique word in document.\n",
        "        document is a list of strings\n",
        "        \"\"\"\n",
        "        word2frequency = {}\n",
        "        for word in document:\n",
        "            word2frequency[word] = (\n",
        "                word2frequency.get(word, 0) + 1\n",
        "            )  # increment, creating key if it doesn't already exist\n",
        "        return word2frequency\n",
        "\n",
        "    def transform(self, collection):\n",
        "        documents = [\n",
        "            document.split() for document in collection\n",
        "        ]  # tokenize documents in the collection\n",
        "        tfidf_mat = np.zeros(\n",
        "            shape=(len(collection), self.count_unique_words)\n",
        "        )  # intialize tfidf matrix with zeros\n",
        "        # compute tfidf\n",
        "        for ind, document in enumerate(documents):\n",
        "            word2frequency = self.tf(document)\n",
        "            for word in word2frequency.keys():\n",
        "                if word in self.word2ind:\n",
        "                    tfidf_mat[ind, self.word2ind[word]] = (\n",
        "                        word2frequency[word] * self.word2idf[word]\n",
        "                    )  # tfidf\n",
        "        return tfidf_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvu2ISM-uckV",
        "outputId": "6ffbbcca-e863-43a6-a569-b1f818cb298e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4246358550964382\n"
          ]
        }
      ],
      "source": [
        "# tfidf of 'the' in last document\n",
        "t = tfidf(documents)\n",
        "tfidf_mat = t.transform(documents)\n",
        "index = t.word2ind[\"the\"]\n",
        "print(tfidf_mat[-1][index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u2-8Oc0HQcj"
      },
      "source": [
        "<h3><b>4. Cosine Similarity</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will compute the cosine similarity of two documents using the TF-IDF representation. The similarity concept is crucial in search engines as well as in text mining applications.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathrm{similarity}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\|v_1\\| \\times \\|v_2\\|}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Normally, the cosine similarity ranges from -1 to +1. However, in our case all the entries of the TF-IDF features are positive, thus it ranges from 0 to 1. The cosine similarity matrix is a square matrix representing the similarity between all pairs of documents in a collection. In other words, if S is the similarity matrix, then $S_{ij} = similarity(v_i , v_j)$ where $v_i$ is the TF-IDF vector of the ith document and $v_j$ is the TF-IDF vector of the jth document.\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OXKsoLZ3Gq3a"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"i like that music\",\n",
        "    \"this song appeals to me\",\n",
        "    \"i like graph theory\",\n",
        "    \"euler is the father of graph theory\",\n",
        "    \"graph theory studies the properties of graphs\",\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"the quick fox jumps over the brown lazy dog\",\n",
        "]\n",
        "\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    \"\"\"returns the cosine similarity of 2 vectors\"\"\"\n",
        "    numerator = np.dot(v1, v2)\n",
        "    denominator = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    return numerator / denominator\n",
        "\n",
        "\n",
        "def cosine_similarity_matrix(mat):\n",
        "    \"\"\"\n",
        "    returns the cosine similarity matrix\n",
        "    the ith row in mat represents the ith vector\n",
        "    \"\"\"\n",
        "    similarity_matrix = np.zeros(shape=(mat.shape[0], mat.shape[0]))\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[0]):\n",
        "            similarity_matrix[i][j] = cosine_similarity(mat[i], mat[j])\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "048fxiP_JSTk",
        "outputId": "adf643f4-6e1d-41ed-a0fe-5860f334667a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.         0.4845028  0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         1.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.4845028  0.         1.         0.28294469 0.28294469 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.28294469 1.         0.39794976 0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.28294469 0.39794976 1.         0.12752163\n",
            "  0.12752163]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]\n",
            " [0.         0.         0.         0.12752163 0.12752163 1.\n",
            "  1.        ]]\n"
          ]
        }
      ],
      "source": [
        "t = tfidf(documents)\n",
        "tfidf_matrix = t.transform(documents)\n",
        "similarity_matrix = cosine_similarity_matrix(tfidf_matrix)\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7QybieKBht"
      },
      "source": [
        "<h3><b>5. Inverted Index</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "Typically, search engines execute queries against a huge document collection.\n",
        "To execute a query we can simply calculate a similarity measure between the query and all the documents in the collection.<br>\n",
        "However, doing so is very inefficient. Instead, it is possible to eliminate a lot of documents from the candidate set by using the inverted index.\n",
        "The inverted index consists of a mapping from words to documents.\n",
        "Each unique word in the vocabulary is mapped to a list containing the documents in which the word appears and the position of the word in these documents. Stemming, which is reducing tokens to a root form, could be useful in this task. For example a stemming algorithm would reduce <i>computing</i>, <i>computers</i> and <i>computation</i> to <i>comput</i> and thus identifying them as one unique token instead of three different ones. In our code we consider two dictionaries to perform the mapping. In one of the dictionaries the words are stemmed, and in the other they are not.\n",
        "Instead of ranking all the documents in the collection, the inverted index allows us to rank only the relevant documents.\n",
        "\n",
        "In our code we provide several functions:\n",
        "<ul>\n",
        "<li><i>at_least_one_unigram()</i>, returns documents containing at least one query word.\n",
        "<li> <i>all_unigrams()</i>, returns documents containing all query words.\n",
        "<li> <i>ngrams()</i>, returns documents containing all query words in the same order as in the query.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "o-fHSDc0JhvQ"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "\n",
        "def clean_tokenize(doc):\n",
        "    words = word_tokenize(doc.lower())\n",
        "    return words\n",
        "\n",
        "\n",
        "def index_one_doc(doc, to_stem):\n",
        "    \"\"\"\n",
        "    creates dict (tok, positions) for each tok in the document (as term list)\n",
        "    \"\"\"\n",
        "    tokpos = dict()\n",
        "    for t_idx, tok in enumerate(doc):\n",
        "        if to_stem:\n",
        "            tok = stemmer.stem(tok)\n",
        "        if tok in tokpos:\n",
        "            tokpos[tok].append(t_idx)\n",
        "        else:\n",
        "            tokpos[tok] = [t_idx]\n",
        "    return tokpos\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = =\n",
        "\n",
        "docs = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The brown quick fox jumps over the lazy dog\",\n",
        "    \"Luke is the mechanical and electrical engineer of the new group\",\n",
        "    \"Instead of buying a new engine, buy a new car\",\n",
        "    \"An engineer may design car engines of all sorts\",\n",
        "    \"Engineers use logic, but not necessarily imagination\",\n",
        "    \"Logic will take you from A to Z, imagination will take you everywhere.\",\n",
        "    \"Continuous effort, not strength or intelligence, is the key to \\\n",
        "        unlocking our potential. And curiosity.\",\n",
        "    \"It’s OK to have your eggs in one basket as long as you control what \\\n",
        "        happens to that basket.\",\n",
        "]\n",
        "\n",
        "cleaned_docs = []\n",
        "for doc in docs:\n",
        "    to_app = clean_tokenize(doc)\n",
        "    cleaned_docs.append(to_app)\n",
        "\n",
        "# = = = = = = = = = = = = = = =\n",
        "\n",
        "index_one_doc(cleaned_docs[3], to_stem=True)\n",
        "\n",
        "index_one_doc(cleaned_docs[3], to_stem=False)\n",
        "\n",
        "\n",
        "# - queries are not case sensitive\n",
        "# - we are indexing punctuation marks\n",
        "# - we index stopwords and should keep stopwords in the queries (gives more expressivity)\n",
        "\n",
        "\n",
        "inverted_index = dict()\n",
        "inverted_index_stem = dict()\n",
        "\n",
        "for d_idx, doc in enumerate(cleaned_docs):\n",
        "\n",
        "    poslists_s = index_one_doc(\n",
        "        doc, to_stem=True\n",
        "    )  # get positions of each token in the doc\n",
        "    for tok, poslist_s in poslists_s.items():\n",
        "        if tok in inverted_index_stem:\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s  # update\n",
        "        else:\n",
        "            inverted_index_stem[tok] = dict()\n",
        "            inverted_index_stem[tok][d_idx] = poslist_s  # initialize\n",
        "\n",
        "    poslists = index_one_doc(doc, to_stem=False)\n",
        "    for tok, poslist in poslists.items():\n",
        "        if tok in inverted_index:\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "        else:\n",
        "            inverted_index[tok] = dict()\n",
        "            inverted_index[tok][d_idx] = poslist\n",
        "\n",
        "\n",
        "# = = = = = = = = = = = = = = = = =\n",
        "\n",
        "\n",
        "def at_least_one_unigram(query, inverted_index):\n",
        "    \"\"\"\n",
        "    returns the indexes of the docs containing *at least one* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    \"\"\"\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.extend(list(inverted_index[unigram].keys()))\n",
        "    return list(set(to_return))\n",
        "\n",
        "\n",
        "def all_unigrams(query, inverted_index):\n",
        "    \"\"\"\n",
        "    returns the indexes of the docs containing *all* query unigrams\n",
        "    the query is a list of unigrams\n",
        "    \"\"\"\n",
        "\n",
        "    to_return = []\n",
        "    for unigram in query:\n",
        "        if unigram in inverted_index:\n",
        "            to_return.append(set(list(inverted_index[unigram].keys())))\n",
        "        else:\n",
        "            to_return.append(set())\n",
        "            break\n",
        "    to_return = to_return[0].intersection(*to_return)\n",
        "    return list(to_return)\n",
        "\n",
        "\n",
        "def ngrams(query, inverted_index):\n",
        "    \"\"\"\n",
        "    returns the indexes of the docs containing all unigrams in same order as the query\n",
        "    the query is a list of unigrams\n",
        "    \"\"\"\n",
        "    candidate_docs = all_unigrams(query, inverted_index)\n",
        "\n",
        "    to_return = []\n",
        "    for doc in candidate_docs:\n",
        "        poslists = []\n",
        "        for unigram in query:\n",
        "            to_append = inverted_index[unigram][doc]\n",
        "            if isinstance(to_append, int):\n",
        "                poslists.append([to_append])\n",
        "            else:\n",
        "                poslists.append(to_append)\n",
        "        # test whether the query words are consecutive\n",
        "        poslists_sub = [\n",
        "            [elt - idx for elt in poslist] for idx, poslist in enumerate(poslists)\n",
        "        ]\n",
        "        if set(poslists_sub[0]).intersection(*poslists_sub):\n",
        "            to_return.append(doc)\n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSfqenBLte7"
      },
      "source": [
        "## Queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5acI7y3yVqnH",
        "outputId": "3729b797-ff94-4a9e-e310-a6a1853902a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: engine car\n",
            "-----------------------------------------------\n",
            "docs containing at least one word in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing all the words in the query:\n",
            "* Instead of buying a new engine, buy a new car\n",
            "\n",
            "docs (stemmed) containing at least one word in the query (stemmed):\n",
            "* Luke is the mechanical and electrical engineer of the new group\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "* Engineers use logic, but not necessarily imagination\n",
            "\n",
            "docs (stemmed) containing all the words in the query (stemmed):\n",
            "* Instead of buying a new engine, buy a new car\n",
            "* An engineer may design car engines of all sorts\n",
            "\n",
            "docs containing all the words in the query in the same order:\n",
            "********************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = [\"engine\", \"car\"]\n",
        "print(\"Query: {}\".format(\" \".join(query)))\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "docs_index = at_least_one_unigram(query, inverted_index)\n",
        "print(\"docs containing at least one word in the query:\")\n",
        "for el in docs_index:\n",
        "    print(\"* {}\".format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query, inverted_index)\n",
        "print(\"docs containing all the words in the query:\")\n",
        "for el in docs_index:\n",
        "    print(\"* {}\".format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "query_stemmed = [stemmer.stem(elt) for elt in query]\n",
        "docs_index = at_least_one_unigram(query_stemmed, inverted_index_stem)\n",
        "print(\"docs (stemmed) containing at least one word in the query (stemmed):\")\n",
        "for el in docs_index:\n",
        "    print(\"* {}\".format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = all_unigrams(query_stemmed, inverted_index_stem)\n",
        "print(\"docs (stemmed) containing all the words in the query (stemmed):\")\n",
        "for el in docs_index:\n",
        "    print(\"* {}\".format(docs[el]))\n",
        "\n",
        "print()\n",
        "\n",
        "docs_index = ngrams(query, inverted_index)\n",
        "print(\"docs containing all the words in the query in the same order:\")\n",
        "for el in docs_index:\n",
        "    print(\"* {}\".format(docs[el]))\n",
        "\n",
        "print(\"********************************************************\")\n",
        "print()\n",
        "\n",
        "tf_idf = tfidf(docs)\n",
        "query = [\"new\", \"car\"]\n",
        "tf_idf_query = tf_idf.transform([(\" \").join(query)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqx4g0guLvED",
        "outputId": "8308e26d-50ba-481f-dbc9-66fcde2fc8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query (new car) over all documents\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 1.1565685272216797 ms\n",
            "\n",
            "Query (new car) over candidates containing at least one of the words\n",
            "-----------------------------------------------\n",
            "Top similar document: Instead of buying a new engine, buy a new car\n",
            "Found in 0.6241798400878906 ms\n"
          ]
        }
      ],
      "source": [
        "############ query over all documents\n",
        "print(\"Query ({}) over all documents\".format((\" \").join(query)))\n",
        "print(\"-----------------------------------------------\")\n",
        "time1 = time.time()\n",
        "tf_idf_collection = tf_idf.transform(docs)\n",
        "scores = [\n",
        "    cosine_similarity(tf_idf_query, tf_idf_collection[i]) for i in range(len(docs))\n",
        "]  # list containing the cosine similarities of the query against all documents\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1) * 1000))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "############ query over candidate documents using inverted index\n",
        "print(\n",
        "    \"Query ({}) over candidates containing at least one of the words\".format(\n",
        "        (\" \").join(query)\n",
        "    )\n",
        ")\n",
        "print(\"-----------------------------------------------\")\n",
        "time1 = time.time()\n",
        "candidate_docs_index = at_least_one_unigram(query, inverted_index)\n",
        "candidate_docs = [docs[el] for el in candidate_docs_index]\n",
        "tf_idf_collection = tf_idf.transform(candidate_docs)\n",
        "scores = [\n",
        "    cosine_similarity(tf_idf_query, tf_idf_collection[i])\n",
        "    for i in range(len(candidate_docs))\n",
        "]  # list containing the cosine similarities of the query against candidate documents\n",
        "time2 = time.time()\n",
        "print(\"Top similar document: {}\".format(candidate_docs[np.argmax(scores)]))\n",
        "print(\"Found in {} ms\".format((time2 - time1) * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R959CcOlNmfn"
      },
      "source": [
        "<h3><b>6. Supervised Classification</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this section we will use the TF-IDF features to perform a supervised classification task. We will work with the WebKB dataset. It features academic webpages belonging to four different categories: (1) project, (2) course, (3) faculty, and (4) students, and contains 2,803 documents for training and 1,396 for testing. Documents have already been preprocessed with stopword removal and Porter stemming. The code that you will work with implements the following steps:\n",
        "\n",
        "<ul>\n",
        "<li>data loading,\n",
        "<li>computation of TF-IDF features for the training set,\n",
        "<li> computation of features for the test set. Note that the documents in the test set are represented in the space made of the unique terms in the training set only (words in the testing set absent from the training set are disregarded).\n",
        "<li>classifier training/testing. Naive Bayes classifier [<a href='https://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf'>McCallum and Nigam, 1998</a>], Multinomial Logistic Regression [<a href='https://www.learningtheory.org/colt2000/papers/CollinsSchapireSinger.pdf'>Collins et al., 2002</a>], Ran- dom Forest Classifier [1] and linear kernel SVM [<a href='https://link.springer.com/article/10.1007/BF00994018'>Cortes and Vapnik 1995</a>, <a href='https://www.researchgate.net/publication/28351286_Text_Categorization_with_Support_Vector_Machines'>Joachims 1998</a>] are compared,\n",
        "<li>get the most/least important words per class.\n",
        "</ul>\n",
        "\n",
        "Then, we test all these functionalities using a small corpus containing a limited number of documents. Finally, we execute a query the naive way (examining all documents) and using the inverted index and we compare the execution time.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaDtszVDNLg8",
        "outputId": "d3142a3f-9302-4edc-e9df-fd352e4c28e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logisitc Regression accuracy: 0.8832378223495702\n",
            "Random forest accuracy: 0.839541547277937\n",
            "Naive Bayes accuracy: 0.8173352435530086\n",
            "Linear SVM accuracy: 0.8474212034383954\n",
            "0.12034383954154727\n",
            "\n",
            "Top 10\n",
            "course: chiang cla guidelin hui instructor data structur syllabu comp fall\n",
            "faculty: csuohio jack recent ufl associ perman teach fax henri professor\n",
            "student: juan graduat icon resum work advisor zhang georg address construct\n",
            "project: technolog lab peopl tool softwar group perform request hybrid high\n",
            "Bottom 10\n",
            "course: cours research berkelei vision interest cpsc graphic bob git\n",
            "faculty: construct georg graduat zhang move riversid syllabu fall icon\n",
            "student: professor henri perman hybrid structur comp fall request data\n",
            "project: address fax interest scienc email offic professor homework home\n"
          ]
        }
      ],
      "source": [
        "def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    # coef stores the weights of each feature (in unique term), for each class\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"%s: %s\" % (class_label, \" \".join(feature_names[j] for j in top10)))\n",
        "\n",
        "\n",
        "def print_bot10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the lowest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        bot10 = np.argsort(clf.coef_[i])[0:9]\n",
        "        print(\"%s: %s\" % (class_label, \" \".join(feature_names[j] for j in bot10)))\n",
        "\n",
        "\n",
        "def prepare_data(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        s = f.read()\n",
        "    examples = s.split(\"\\n\")  # split to samples\n",
        "    examples = examples[:-1]  # last element is empty\n",
        "    documents = []\n",
        "    labels = []\n",
        "    for el in examples:\n",
        "        example = el.split(\"\\t\")  # separate document from label\n",
        "        documents.append(example[1])\n",
        "        labels.append(example[0])\n",
        "    return documents, labels\n",
        "\n",
        "\n",
        "path_train = \"./webkb-train-stemmed.txt\"  # path to train data\n",
        "path_test = \"./webkb-test-stemmed.txt\"  # path to test data\n",
        "train_documents, train_labels = prepare_data(path_train)\n",
        "test_documents, test_labels = prepare_data(path_test)\n",
        "\n",
        "categories = set(train_labels)  # get unique categoris\n",
        "category2ind = dict(\n",
        "    zip(categories, range(len(categories)))\n",
        ")  # map each category to index\n",
        "ind2category = {v: k for k, v in category2ind.items()}  # map index to category\n",
        "\n",
        "train_labels_index = [\n",
        "    category2ind[cat] for cat in train_labels\n",
        "]  # replace labels by their indexes\n",
        "test_labels_index = [\n",
        "    category2ind[cat] for cat in test_labels\n",
        "]  # replace labels by their indexes\n",
        "\n",
        "t = tfidf(train_documents)  # tfidf object\n",
        "tfidf_train = t.transform(train_documents)  # get the tfidf training matrix\n",
        "tfidf_test = t.transform(test_documents)  # get the tfidf text matrix\n",
        "\n",
        "lr = LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=200)\n",
        "rf = RandomForestClassifier(n_estimators=20)\n",
        "nb = MultinomialNB()\n",
        "svm = LinearSVC(max_iter=2000)\n",
        "\n",
        "Classifiers = {\n",
        "    \"Logisitc Regression\": lr,\n",
        "    \"Random forest\": rf,\n",
        "    \"Naive Bayes\": nb,\n",
        "    \"Linear SVM\": svm,\n",
        "}\n",
        "\n",
        "for classifier in Classifiers.keys():\n",
        "    Classifiers[classifier].fit(\n",
        "        tfidf_train, train_labels_index\n",
        "    )  # train each classifier\n",
        "    predicted = Classifiers[classifier].predict(tfidf_test)  # perform prediction\n",
        "    accuracy = np.mean(predicted == test_labels_index)  # compute accuracy\n",
        "    print(\"{} accuracy: {}\".format(classifier, accuracy))\n",
        "\n",
        "predicted = np.zeros((len(test_labels_index))) + 3\n",
        "print(np.mean(predicted == np.array(test_labels_index)))\n",
        "print(\"\")\n",
        "\n",
        "# choose one classfier and print top 10 and bottom 10 words per class\n",
        "feature_names = t.getListWords()\n",
        "classifier = svm\n",
        "print(\"Top 10\")\n",
        "print_top10(feature_names, classifier, categories)\n",
        "print(\"Bottom 10\")\n",
        "print_bot10(feature_names, classifier, categories)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10 (main, Jan 15 2022, 11:40:53) \n[Clang 13.0.0 (clang-1300.0.29.3)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

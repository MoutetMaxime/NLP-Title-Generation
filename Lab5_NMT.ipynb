{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center>\n",
        "<h1>\n",
        "<h1>INF582: INTRODUCTION TO TEXT MINING AND NLP</h1>\n",
        "<h2>Lab Session 5: Neural Machine Translation</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<h3><b>1. Learning Objective:</b></h2>\n",
        "<p style=\"text-align: justify;\">\n",
        "In this lab, you will learn about sequence to sequence (seq2seq) architectures.\n",
        "More precisely, we will implement the Neural Machine Translation (NMT) model described in <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> using Python 3.6 and PyTorch (the latest version).\n",
        "The only difference is that we will be using non-stacked RNNs, whereas <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> uses stacked RNNs.\n",
        "\n",
        "We will train our model on the task of English to French translation, using a set of sentence pairs from <a href='http://www.manythings.org/anki/'>http://www.manythings.org/anki/</a>, originally extracted from the Tatoeba project: <a href='https://tatoeba.org/eng/'>https://tatoeba.org/eng/</a>.\n",
        "\n",
        "Our dataset features 136,521 pairs for training and 34,130 pairs for testing, which is quite small, but enough for the purpose of this lab.\n",
        "The average size of a source sentence is 7.6 while the average size of a target sentence is 8.3.\n",
        "\n",
        "$\\underline{\\textbf{Note}}$: the pairs have already been preprocessed.\n",
        "Each sentence was turned into a list of integers starting from 4.\n",
        "The integers correspond to indexes in the source and target vocabularies, that have been constructed from the training set, and in which the most frequent words have index 4.\n",
        "0, 1, 2 and 3 are reserved respectively for the padding, out-of-vocabulary, start of sentence, and end of sentence special tokens.\n",
        "\n",
        "<h3><b>2. Recurrent Neural Networks:</b></h3>\n",
        "<p style=\"text-align: justify;\">\n",
        "\n",
        "While CNNs are good at dealing with grids, RNNs were specifically developed to be used with sequences.\n",
        "As shown in Fig. 1, a RNN can be viewed as a chain of simple neural layers that share the same parameters.\n",
        "From a high level, a RNN is fed an ordered list of input vectors $\\big\\{x_{1},...,x_{T}\\big\\}$ as well as an initial hidden state $h_{0}$ initialized to all zeros, and returns an ordered list of hidden states $\\big\\{h_{1},...,h_{T}\\big\\}$, as well as an ordered list of output vectors $\\big\\{y_{1},...,y_{T}\\big\\}$.\n",
        "The hidden states may serve as input to the RNN units above in the case of a stacked architecture, or directly be used as they are (e.g., by the attention mechanism).\n",
        "The hidden states correspond more or less to the \"short-term\" memory of the network.\n",
        "<center>\n",
        "<table><tr>\n",
        "<td> <img src='https://am3pap003files.storage.live.com/y4mADPYPHjBcGTnT7CZP44m-lKWhTWskrhuFQdYx35RuUSnLvcnxNWwTkV1yuTgYeneKprJ2xb1G2pQ4JDnLEKk-Ze4-Sig34iqPml_V6edc9_bFC45sR7s1o1FsJyETpKoHRGNFaUqkHPk7G1an2FEWmutoKskWiIlRFVG-QRnPTrb1NDGHF3A1utmLhOxVC_W?width=498&height=246&cropmode=none' alt=\"Drawing\" width= '500px'/> </td>\n",
        "<td> <img src=\"https://am3pap003files.storage.live.com/y4mnv0LzVyEhrZ8VQG5zMAw1yTajHpJ-vQGkjqifaKvaRC4BhGn73XsmcUg67ykmYL-1031mxHc6hVdA8bjJFhvfH_4vofGLJStrnw7euQQzUa2GZvNLk-FiLZbB5IXHIA7pxnigowEWvxhEhsFsSN_TVCx7DYX3FIVpUv4DfM49oGLG3u94YC-YS5x21Jx7jbp?width=1336&height=733&cropmode=none\" alt=\"Drawing\" width='500px'/> </td>\n",
        "</tr></table>\n",
        "\n",
        "<b>Figure 1:</b> Left: 3 steps of an unrolled RNN (adapted from <a href='http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/'>Denny Britz' blog</a>). Right: 3 steps of an unrolled stacked RNN.\n",
        "The hidden states at a given position flow vertically through the RNN layers. On both sides, each circle represents a  RNN unit. </a><br>\n",
        "</center>\n",
        "\n",
        "<h3><b>3. Sequence-to-sequence architecture:</b></h3>\n",
        "<p style=\"text-align: justify;\">\n",
        "Our input and output are sequences of words, respectively $x = \\big(x_1, \\dots ,x_{T_x}\\big)$ and $y = \\big(y_1, \\dots ,y_{T_y}\\big)$.\n",
        "$x$ and $y$ are usually referred to as the $\\textit{source}$ and $\\textit{target}$ sentences.\n",
        "<h4><b>3.1. Encoder</b></h4>\n",
        "<p style=\"text-align: justify;\">\n",
        "Our encoder is a non-stacked unidirectional RNN with GRU units (see the appendix for details about the GRU.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB6pvLvlKbtD",
        "outputId": "82de9813-9aac-4b9c-8972-91df0b99d107"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZgoCyV27q65"
      },
      "source": [
        "<h4><b>3.2. Decoder</b></h4>\n",
        "<p style=\"text-align: justify;\">\n",
        "Our decoder is a non-stacked unidirectional RNN.\n",
        "It is a neural language model conditioned not only on the previously generated target words but also on the source sentence.\n",
        "More precisely, it generates the target sentence $y=(y_1,\\dots,y_{T_y})$ one word $y_t$ at a time based on the distribution:\n",
        "\n",
        "\\begin{equation}\n",
        "P\\big[y_t|\\{y_{1},...,y_{t-1}\\},c_t\\big] = \\mathrm{softmax}\\big(W_s\\tilde{h}_t\\big)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\tilde{h}_t$, the \\textit{attentional} hidden state, is computed as (biases are not shown for simplicity):\n",
        "\n",
        "\\begin{equation}\n",
        "\\tilde{h}_t = \\mathrm{tanh}\\big(W_c\\big[c_t;h_t\\big]\\big)\n",
        "\\end{equation}\n",
        "\n",
        " $h_t$ is the $t^{th}$ hidden state of the decoder, $c_t$ is the source context vector, and $\\big[;\\big]$ denotes concatenation. $W_s$ and $W_c$ are matrices of trainable parameters.\n",
        "\n",
        "$\\textbf{Note:}$ while all the inputs of the encoder (i.e., all the words of the input sentence) are known at encoding time, the decoder generates one target word at a time, and uses as input at time $t$ its prediction from time $t-1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"to be used one timestep at a time\n",
        "    see https://pytorch.org/docs/stable/nn.html#gru\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, source_context, h):\n",
        "        output, h_t = self.rnn(self.embedding(input), h)\n",
        "        concatenated_output = torch.cat((output, source_context.unsqueeze(0)), dim=2)\n",
        "        tilde_h_t = torch.tanh(self.ff_concat(concatenated_output))\n",
        "        prediction = self.predict(tilde_h_t)\n",
        "        return prediction, h_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mE7IkYx8Kjl"
      },
      "source": [
        "<h4><b>3.3. Global attention mechanism</b></h4>\n",
        "<p style=\"text-align: justify;\">\n",
        "The context vector $c_t$ is computed as a weighted sum of the encoder's hidden states $\\bar{h}_i$.\n",
        "The vector of weights $\\alpha_{t}$ is obtained by applying a softmax to the output of an $\\textit{alignment}$ operation ($\\texttt{score()}$) between the current target hidden state $h_t$ and all source hidden states $\\bar{h}_{i}$'s.\n",
        "$\\alpha_{t}$ indicates which words in the source sentence are the most likely to help in predicting the next word.\n",
        "$\\texttt{score()}$ can in theory be any comparison function.\n",
        "In our implementation, we will use the $\\texttt{concat}$ attention formulation of <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> (see section 3.1 of the paper).\n",
        "An overview is provided in Fig. 2.  at a time, and uses as input at time $t$ its prediction from time $t-1$.\n",
        "<center>\n",
        "<img width='800px' src='https://am3pap003files.storage.live.com/y4mp9luLosWxfQX_f93pUgybccFBrF5TSh3uB41wBKzNeaL7fDlICHP4q9Vpr3qTmriWwUsJcTC2nI-IKW9rsv3Wa-b2vVo1eesKbm9gixxIaqEiIZ585xPFK-Hsnc896qELG8jTqPVrTY4YYhQ4iPVfIksVYRg1J-7BcZDlorToWiQJZnZf4Omg7BnVMeQhk6-?width=1836&height=874&cropmode=none' />\n",
        "<br>\n",
        "<b>Figure 2:</b>Summary of the $\\textit{global attention}$ mechanism <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> <br>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "outputs": [],
      "source": [
        "class seq2seqAtt(nn.Module):\n",
        "    \"\"\"\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s + hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False)  # just a dot product here\n",
        "\n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(\n",
        "            source_hs.size(0), 1, 1\n",
        "        )  # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_hs = torch.cat((source_hs, target_h_rep), dim=2)\n",
        "        concat_output = self.ff_concat(concat_hs)\n",
        "        scores = self.ff_score(\n",
        "            torch.tanh(concat_output)\n",
        "        )  # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(\n",
        "            dim=2\n",
        "        )  # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, 0)\n",
        "        source_hs_p = source_hs.permute(\n",
        "            (2, 0, 1)\n",
        "        )  # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (\n",
        "            norm_scores * source_hs_p\n",
        "        )  # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(\n",
        "            weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True\n",
        "        )  # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes\n",
        "        return ct, norm_scores.squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLurQq5qABAz"
      },
      "source": [
        "<h3><b>4. Training and Evaluation:</b></h3>\n",
        "<p style=\"text-align: justify;\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "outputs": [],
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    \"\"\"the full seq2seq model\"\"\"\n",
        "\n",
        "    ARGS = [\n",
        "        \"vocab_s\",\n",
        "        \"source_language\",\n",
        "        \"vocab_t_inv\",\n",
        "        \"embedding_dim_s\",\n",
        "        \"embedding_dim_t\",\n",
        "        \"hidden_dim_s\",\n",
        "        \"hidden_dim_t\",\n",
        "        \"hidden_dim_att\",\n",
        "        \"do_att\",\n",
        "        \"padding_token\",\n",
        "        \"oov_token\",\n",
        "        \"sos_token\",\n",
        "        \"eos_token\",\n",
        "        \"max_size\",\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_s,\n",
        "        source_language,\n",
        "        vocab_t_inv,\n",
        "        embedding_dim_s,\n",
        "        embedding_dim_t,\n",
        "        hidden_dim_s,\n",
        "        hidden_dim_t,\n",
        "        hidden_dim_att,\n",
        "        do_att,\n",
        "        padding_token,\n",
        "        oov_token,\n",
        "        sos_token,\n",
        "        eos_token,\n",
        "        max_size,\n",
        "    ):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att  # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print(\"max source index\", self.max_source_idx)\n",
        "        print(\"source vocab size\", len(vocab_s))\n",
        "\n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print(\"max target index\", self.max_target_idx)\n",
        "        print(\"target vocab size\", len(vocab_t_inv))\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.encoder = Encoder(\n",
        "            self.max_source_idx + 1,\n",
        "            self.embedding_dim_s,\n",
        "            self.hidden_dim_s,\n",
        "            self.padding_token,\n",
        "        ).to(self.device)\n",
        "        self.decoder = Decoder(\n",
        "            self.max_target_idx + 1,\n",
        "            self.embedding_dim_t,\n",
        "            self.hidden_dim_t,\n",
        "            self.padding_token,\n",
        "        ).to(self.device)\n",
        "\n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(\n",
        "                self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t\n",
        "            ).to(self.device)\n",
        "\n",
        "    def my_pad(self, my_list):\n",
        "        \"\"\"my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence\"\"\"\n",
        "        batch_source = pad_sequence(\n",
        "            [\n",
        "                torch.cat((elt[0], torch.LongTensor([self.eos_token])))\n",
        "                for elt in my_list\n",
        "            ],\n",
        "            batch_first=True,\n",
        "            padding_value=self.padding_token,\n",
        "        )\n",
        "        batch_target = pad_sequence(\n",
        "            [\n",
        "                torch.cat((elt[1], torch.LongTensor([self.eos_token])))\n",
        "                for elt in my_list\n",
        "            ],\n",
        "            batch_first=True,\n",
        "            padding_value=self.padding_token,\n",
        "        )\n",
        "        return batch_source, batch_target\n",
        "\n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod:\n",
        "            input = input.unsqueeze(\n",
        "                1\n",
        "            )  # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder(input)\n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(\n",
        "            self.device\n",
        "        )  # init (1, batch, feat)\n",
        "\n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = (\n",
        "            torch.LongTensor([self.sos_token])\n",
        "            .repeat(current_batch_size)\n",
        "            .unsqueeze(0)\n",
        "            .to(self.device)\n",
        "        )  # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        weights = []\n",
        "\n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                source_context, att_weights = self.att_mech(target_h, source_hs)\n",
        "                # FILL THE GAP FOR QUESTION 3 (Not Necessary for TASK4): fill \"weights\" variable\n",
        "                weights.append(att_weights)\n",
        "                source_context = source_context[0]  # (1, batch, feat)\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(\n",
        "                    0\n",
        "                )  # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
        "            logits.append(prediction)  # (1, batch, vocab)\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            target_input = prediction.argmax(dim=2)\n",
        "            eos_counter += torch.sum(target_input == self.eos_token).item()\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(\n",
        "            logits, 0\n",
        "        )  # logits is a list of tensors -> (seq, batch, vocab)\n",
        "\n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1)  # (seq, vocab)\n",
        "\n",
        "        return to_return, weights\n",
        "\n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(\n",
        "            ignore_index=self.padding_token\n",
        "        )  # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(\n",
        "            trainingDataset, batch_size=batch_size, shuffle=True, collate_fn=self.my_pad\n",
        "        )  # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(\n",
        "            testDataset, batch_size=512, collate_fn=self.my_pad\n",
        "        )\n",
        "        tdqm_dict_keys = [\"loss\", \"test loss\"]\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            with tqdm(\n",
        "                total=len(train_loader),\n",
        "                unit_scale=True,\n",
        "                postfix={\"loss\": 0.0, \"test loss\": 0.0},\n",
        "                desc=\"Epoch : %i/%i\" % (epoch, n_epochs - 1),\n",
        "                ncols=100,\n",
        "            ) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(\n",
        "                            self.device\n",
        "                        )  # RNN needs (seq, batch, feat) but loader returns (batch, seq)\n",
        "                        batch_target = batch_target.transpose(1, 0).to(\n",
        "                            self.device\n",
        "                        )  # (seq, batch)\n",
        "\n",
        "                        # are we using the model in production\n",
        "                        is_prod = (\n",
        "                            len(batch_source.shape) == 1\n",
        "                        )  # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(\n",
        "                                0\n",
        "                            )  # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "\n",
        "                        unnormalized_logits = self.forward(\n",
        "                            batch_source, max_size, is_prod\n",
        "                        )[0]\n",
        "                        sentence_loss = criterion(\n",
        "                            unnormalized_logits.flatten(end_dim=1),\n",
        "                            batch_target.flatten(),\n",
        "                        )\n",
        "                        total_loss += sentence_loss.item()\n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss / (i + 1)\n",
        "                        pbar.set_postfix(tdqm_dict)\n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad()  # flush gradient attributes\n",
        "                            sentence_loss.backward()  # compute gradients\n",
        "                            optimizer.step()  # update\n",
        "                            pbar.update(1)\n",
        "\n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1  # reset\n",
        "\n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "\n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        \"\"\"converts natural language source sentence into source integers\"\"\"\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\", \" \").replace(\"-\", \" \")\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [\n",
        "            int(self.vocab_s[elt]) if elt in self.vocab_s else self.oov_token\n",
        "            for elt in source_nl_clean_tok\n",
        "        ]\n",
        "\n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints\n",
        "\n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        \"\"\"converts integer target sentence into target natural language\"\"\"\n",
        "        return [\n",
        "            (\n",
        "                \"<PAD>\"\n",
        "                if elt == self.padding_token\n",
        "                else (\n",
        "                    \"<OOV>\"\n",
        "                    if elt == self.oov_token\n",
        "                    else (\n",
        "                        \"<EOS>\"\n",
        "                        if elt == self.eos_token\n",
        "                        else \"<SOS>\" if elt == self.sos_token else self.vocab_t_inv[elt]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            for elt in target_ints\n",
        "        ]\n",
        "\n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits = self.forward(\n",
        "            source_ints, self.max_size, True\n",
        "        )  # (seq) -> (<=max_size, vocab)\n",
        "        target_ints = logits[0].argmax(-1).squeeze()  # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return \" \".join(target_nl), logits[1]\n",
        "\n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr: getattr(self, attr) for attr in self.ARGS}\n",
        "        attrs[\"state_dict\"] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "\n",
        "    @classmethod  # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(\n",
        "            path_to_file, map_location=lambda storage, loc: storage\n",
        "        )  # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop(\"state_dict\")\n",
        "        new = cls(**attrs)  # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-HH53wAq3S"
      },
      "source": [
        "### Download the data and the pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "datl5SFtJ9Br",
        "outputId": "11ec46be-4b35-475c-f74b-c188708d424d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            " extracting: pairs_test_ints.txt     \n",
            " extracting: pairs_train_ints.txt    \n",
            " extracting: README.txt              \n",
            " extracting: vocab_source.json       \n",
            " extracting: vocab_target.json       \n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\", \"data.zip\")\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\", \"pretrained_moodle.pt\")\n",
        "!unzip data.zip\n",
        "\n",
        "path_to_data = './'\n",
        "path_to_save_models = './'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmBgproQAv8_"
      },
      "source": [
        "### Define the dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "outputs": [],
      "source": [
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)  # total nb of observations\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx]  # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(\n",
        "        path_to_data + \"pairs_\" + train_or_test + \"_ints.txt\", \"r\", encoding=\"utf-8\"\n",
        "    ) as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split(\"\\t\") for elt in pairs_tmp]\n",
        "    pairs_tmp = [\n",
        "        [[int(eltt) for eltt in elt[0].split()], [int(eltt) for eltt in elt[1].split()]]\n",
        "        for elt in pairs_tmp\n",
        "    ]\n",
        "    return pairs_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSZ-cvSuLQVt",
        "outputId": "1250f6b6-89ce-4d65-b27e-eea3d14f0f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data loaded\n",
            "data prepared\n",
            "= = = attention-based model?: True = = =\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch : 0/19: 100%|█████████████████| 2.13k/2.13k [01:44<00:00, 20.4it/s, loss=5.12, test loss=4.54]\n",
            "Epoch : 1/19: 100%|█████████████████| 2.13k/2.13k [01:46<00:00, 20.0it/s, loss=4.26, test loss=4.04]\n",
            "Epoch : 2/19: 100%|█████████████████| 2.13k/2.13k [01:51<00:00, 19.1it/s, loss=3.88, test loss=3.74]\n",
            "Epoch : 3/19: 100%|█████████████████| 2.13k/2.13k [01:43<00:00, 20.7it/s, loss=3.63, test loss=3.55]\n",
            "Epoch : 4/19: 100%|█████████████████| 2.13k/2.13k [01:41<00:00, 21.0it/s, loss=3.44, test loss=3.41]\n",
            "Epoch : 5/19: 100%|██████████████████| 2.13k/2.13k [01:44<00:00, 20.5it/s, loss=3.3, test loss=3.28]\n",
            "Epoch : 6/19: 100%|██████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=3.18, test loss=3.2]\n",
            "Epoch : 7/19: 100%|█████████████████| 2.13k/2.13k [01:43<00:00, 20.6it/s, loss=3.09, test loss=3.12]\n",
            "Epoch : 8/19: 100%|█████████████████| 2.13k/2.13k [01:47<00:00, 19.8it/s, loss=3.01, test loss=3.07]\n",
            "Epoch : 9/19: 100%|█████████████████| 2.13k/2.13k [01:47<00:00, 19.8it/s, loss=2.94, test loss=3.01]\n",
            "Epoch : 10/19: 100%|████████████████| 2.13k/2.13k [01:43<00:00, 20.6it/s, loss=2.89, test loss=2.97]\n",
            "Epoch : 11/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 21.0it/s, loss=2.84, test loss=2.93]\n",
            "Epoch : 12/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 21.0it/s, loss=2.79, test loss=2.89]\n",
            "Epoch : 13/19: 100%|████████████████| 2.13k/2.13k [01:42<00:00, 20.9it/s, loss=2.75, test loss=2.87]\n",
            "Epoch : 14/19: 100%|████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=2.72, test loss=2.83]\n",
            "Epoch : 15/19: 100%|████████████████| 2.13k/2.13k [01:42<00:00, 20.9it/s, loss=2.69, test loss=2.81]\n",
            "Epoch : 16/19: 100%|████████████████| 2.13k/2.13k [01:43<00:00, 20.5it/s, loss=2.66, test loss=2.79]\n",
            "Epoch : 17/19: 100%|████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=2.63, test loss=2.78]\n",
            "Epoch : 18/19: 100%|█████████████████| 2.13k/2.13k [01:43<00:00, 20.5it/s, loss=2.6, test loss=2.78]\n",
            "Epoch : 19/19: 100%|████████████████| 2.13k/2.13k [01:43<00:00, 20.6it/s, loss=2.58, test loss=2.75]\n"
          ]
        }
      ],
      "source": [
        "do_att = True  # should always be set to True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pairs_train = load_pairs(\"train\")\n",
        "pairs_test = load_pairs(\"test\")\n",
        "\n",
        "with open(path_to_data + \"vocab_source.json\", \"r\") as file:\n",
        "    vocab_source = json.load(file)  # word -> index\n",
        "\n",
        "with open(path_to_data + \"vocab_target.json\", \"r\") as file:\n",
        "    vocab_target = json.load(file)  # word -> index\n",
        "\n",
        "vocab_target_inv = {v: k for k, v in vocab_target.items()}  # index -> word\n",
        "print(\"data loaded\")\n",
        "training_set = Dataset(pairs_train)\n",
        "test_set = Dataset(pairs_test)\n",
        "print(\"data prepared\")\n",
        "print(\"= = = attention-based model?:\", str(do_att), \"= = =\")\n",
        "\n",
        "model = seq2seqModel(\n",
        "    vocab_s=vocab_source,\n",
        "    source_language=\"english\",\n",
        "    vocab_t_inv=vocab_target_inv,\n",
        "    embedding_dim_s=40,\n",
        "    embedding_dim_t=40,\n",
        "    hidden_dim_s=30,\n",
        "    hidden_dim_t=30,\n",
        "    hidden_dim_att=20,\n",
        "    do_att=do_att,\n",
        "    padding_token=0,\n",
        "    oov_token=1,\n",
        "    sos_token=2,\n",
        "    eos_token=3,\n",
        "    max_size=30,\n",
        ").to(\n",
        "    device\n",
        ")  # max size of generated sentence in prediction mode\n",
        "\n",
        "model.fit(training_set, test_set, lr=0.001, batch_size=64, n_epochs=20, patience=2)\n",
        "model.save(path_to_save_models + \"my_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwUInZMyQzci",
        "outputId": "2b153fd3-b1b8-4491-9019-55965a7a2a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " I am a student. -> je suis étudiant . . . <EOS>\n",
            "= = = = = \n",
            " I have a red car. -> je ai une voiture voiture . . <EOS>\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer de jeux jeux . . . <EOS>\n",
            "= = = = = \n",
            " This river is full of fish. -> c rivière est plein de poisson . . <EOS>\n",
            "= = = = = \n",
            " The fridge is full of food. -> le réfrigérateur est plein de nourriture . . . <EOS>\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat est endormi sur le . . . <EOS>\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime aime pizza . . <EOS>\n",
            "= = = = = \n",
            " I did not mean to hurt you -> je n ai pas pas dire à vous blesser blesser blesser <EOS>\n",
            "= = = = = \n",
            " She is so mean -> elle est si dire dire dire . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi m une cravate cravate à à avec ce ce ce <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed -> je ne peux pas aider de de fumer de de fumer fumer fumer <EOS>\n",
            "= = = = = \n",
            " The kids were playing hide and seek -> les enfants ont et et et <OOV> <OOV> <EOS>\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace -> le chat est endormi sa . . . <EOS>\n"
          ]
        }
      ],
      "source": [
        "model = seq2seqModel.load(path_to_save_models + \"my_model.pt\")\n",
        "to_test = [\n",
        "    \"I am a student.\",\n",
        "    \"I have a red car.\",  # inversion captured\n",
        "    \"I love playing video games.\",\n",
        "    \"This river is full of fish.\",  # plein vs pleine (accord)\n",
        "    \"The fridge is full of food.\",\n",
        "    \"The cat fell asleep on the mat.\",\n",
        "    \"my brother likes pizza.\",  # pizza is translated to 'la pizza'\n",
        "    \"I did not mean to hurt you\",  # translation of mean in context\n",
        "    \"She is so mean\",\n",
        "    \"Help me pick out a tie to go with this suit!\",  # right translation\n",
        "    \"I can't help but smoking weed\",  # this one and below: hallucination\n",
        "    \"The kids were playing hide and seek\",\n",
        "    \"The cat fell asleep in front of the fireplace\",\n",
        "]\n",
        "\n",
        "for elt in to_test:\n",
        "    print(\"= = = = = \\n\", \"%s -> %s\" % (elt, model.predict(elt)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXH6rq1w3myo",
        "outputId": "930e4546-f96e-4b3b-aad1-688ba866062c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([0.5947, 0.2282, 0.0713, 0.0853, 0.0204], device='cuda:0',\n",
              "        grad_fn=<SqueezeBackward1>),\n",
              " tensor([0.1780, 0.3954, 0.1207, 0.2643, 0.0415], device='cuda:0',\n",
              "        grad_fn=<SqueezeBackward1>),\n",
              " tensor([0.0030, 0.0177, 0.1338, 0.6460, 0.1995], device='cuda:0',\n",
              "        grad_fn=<SqueezeBackward1>),\n",
              " tensor([4.8130e-04, 6.8082e-04, 4.5676e-03, 4.2926e-01, 5.6501e-01],\n",
              "        device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
              " tensor([0.0044, 0.0066, 0.0146, 0.2259, 0.7485], device='cuda:0',\n",
              "        grad_fn=<SqueezeBackward1>),\n",
              " tensor([0.0201, 0.0200, 0.0471, 0.3118, 0.6010], device='cuda:0',\n",
              "        grad_fn=<SqueezeBackward1>),\n",
              " tensor([0.0348, 0.0247, 0.0478, 0.2672, 0.6255], device='cuda:0',\n",
              "        grad_fn=<SqueezeBackward1>)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(\"I am a student.\")[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "K4eaYTs3E4gQ",
        "outputId": "02cb94ec-7807-4217-b1d1-03fc3cf6c06e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-92bde67f3baf>:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + source_tokens, rotation=90, fontsize=10)\n",
            "<ipython-input-27-92bde67f3baf>:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + target_tokens, fontsize=10)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_alignment(attention_weights, source_tokens, target_tokens):\n",
        "    \"\"\"\n",
        "    Plots an alignment visualization with attention weights.\n",
        "\n",
        "    :param attention_weights: A 2D numpy array of shape (target_length, source_length)\n",
        "                              containing the attention weights.\n",
        "    :param source_tokens: A list of source tokens.\n",
        "    :param target_tokens: A list of target tokens.\n",
        "    :param gold_alignment: A list of tuples indicating the gold-standard alignments.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    cax = ax.matshow(attention_weights, cmap=\"gray_r\", aspect=\"auto\")\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([\"\"] + source_tokens, rotation=90, fontsize=10)\n",
        "    ax.set_yticklabels([\"\"] + target_tokens, fontsize=10)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "\n",
        "    plt.xlabel(\"Source Sequence\")\n",
        "    plt.ylabel(\"Target Sequence\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "prediction = model.predict(\"I am a student.\")[1]\n",
        "source_tokens = [\"<sos>\", \"I\", \"am\", \"a\", \"student\", \"<eos>\"]\n",
        "target_tokens = [\"<sos>\", \"Je\", \"suis\", \"un\", \"étudiant\", \"<eos>\"]\n",
        "attention_weights = torch.Tensor(\n",
        "    [prediction[i].detach().cpu().numpy() for i in range(len(prediction))]\n",
        ")\n",
        "\n",
        "plot_alignment(attention_weights, source_tokens, target_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z--prNCiJ0Un"
      },
      "source": [
        "<h3><b>6. Appendix:</b></h3>\n",
        "<h4><b>6.1. GRU unit:</b></h4>\n",
        "<p style=\"text-align: justify;\">\n",
        "As shown in Fig. 3, the GRU unit <a href='https://arxiv.org/abs/1406.1078'>[Cho et al., 2014]</a> is a simple RNN unit with two gates (reset and update):\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{reset gate:}~~r_{t} = \\sigma \\big(U_{r}x_{t} + W_{r}h_{t-1} + b_r\\big)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{update gate:}~~z_{t} = \\sigma \\big(U_{z}x_{t} + W_{z}h_{t-1} + b_z\\big)\n",
        "\\end{equation}\n",
        "\n",
        "The candidate hidden state is computed as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{h}_{t} = \\mathrm{tanh} \\big(U_{h}x_{t} + W_{h} (r_t \\circ h_{t-1}) + b_h\\big)\n",
        "\\end{equation}\n",
        "\n",
        "<center>\n",
        "<img width='500px' src='https://am3pap003files.storage.live.com/y4mugznZ4hrS1IJ0VQuynYnVaEFVK8-3_YEEDL3pzmbtsCqzdADi8xJf5KFEA55E5cpQt36PH_D-223lncXjrBrDWE5Tke-pn-LO_Sb1KifdEz5vmUJkTzmTAiUdn1DN_hDS9aToU-TpnnarZwzYmvd6zKqaatUEFM8Dale7-2StQJ-owZawYr3r5O5H_PHzMod?width=703&height=489&cropmode=none' />\n",
        "<br>\n",
        "<b>Figure 3:</b> GRU unit. Taken from <a href='http://colah.github.io/posts/2015-08-Understanding-LSTMs/'>Chris Olah's blog</a>.<br>\n",
        "</center>\n",
        "\n",
        "The reset gate determines how much of the information from the previous time steps (stored in $h_{t-1}$) should be discarded.\n",
        "The new hidden state is finally obtained by linearly interpolating between the previous hidden state and the candidate one:\n",
        "\n",
        "\\begin{equation}\n",
        "h_{t} = (1-z_t) \\circ {h}_{t-1} + z_t \\circ \\hat{h}_{t}\n",
        "\\end{equation}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10 (main, Jan 15 2022, 11:40:53) \n[Clang 13.0.0 (clang-1300.0.29.3)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

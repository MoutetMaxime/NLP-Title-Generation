{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labelling\n",
    "\n",
    "In this session we will build an HMM model for PoS-tagging and then CRF and neural models for Named Entity Recognition.\n",
    "\n",
    "## Building a simple Hidden Markov Model\n",
    "\n",
    "In this first part of the lab we will build a very simple bigram HMM using probability estimates over the Brown corpus, which is Part-of-Speech tagged.\n",
    "\n",
    "Recall from course 6: probability estimates (with MLE estimation) can be calculated by dividing the number of occurrences of a bigram by the number of occurrences of the first word.\n",
    "\n",
    "First of all, we import the corpus where we will estimate the probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus is in the form of sequences of sentences, where each sentence is made by a sequence of pairs (word, POS-tag), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/maximemoutet/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"brown\")\n",
    "brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall (from the course) that a Hidden Markov Model is composed by:\n",
    "\n",
    "- A set of $N$ states $Q = \\{q_1, q_2, \\ldots, q_N\\}$\n",
    "- A transition probability matrix $A=a_{11}\\ldots a_{ij} \\ldots a_{NN}$, where each $a_{ij}$ represents the probability of transitioning from state $q_i$ to $q_j$; note that $\\sum_{j=1}^N{a_{ij}} = 1 \\forall i$\n",
    "- A sequence of $T$ observations $O = o_1, o_2, \\ldots o_T$, each one drawn from a vocabulary of size $V=v_1, v_2, \\ldots, v_M$, of size $M$;\n",
    "- A sequence of *observation likelihoods* $B=b_i(o_t)$, also called **emission probabilities**, each expressing the probability of an observation $o_t$ being generated from a state $q_i$;\n",
    "- Finally, an initial probability distribution $\\Pi = \\pi_i, \\pi_2, \\ldots, \\pi_N$ where $\\pi_i$ indicates the probability that the Markov chain will start from state $q_i$. Some states $q_j$ may have $\\pi_j = 0$, meaning that they cannot be initial states. Also, $\\sum_{i=1}^N{\\pi_i}=1$.\n",
    "\n",
    "In our case, the set of states $Q$ is made by the vocabulary of labels (the POS-tags). The vocabulary $V$ corresponds to the word vocabulary (i.e. all the set of different words that appear in our corpus). The observations correspond to the sentences in the corpus.\n",
    "\n",
    "We will now split our corpus in a training and test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents()\n",
    "\n",
    "training = corpus[:-10]\n",
    "testing = corpus[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Extract $Q$ and $V$ from the Brown corpus and determine their respective size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V = set(), set()\n",
    "\n",
    "for sentence in corpus:\n",
    "    for word, tag in sentence:\n",
    "        Q.add(tag)\n",
    "        V.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Create the matrices ($A$, $B$ and $\\Pi$) by using the probabilities estimated on the training set; since we are considering bigrams, the probabilities of the transition matrix will be calculated as $\\frac{count(t_{-1}, t)}{count(t_{-1})}$.\n",
    "\n",
    "*Important*: you will need to add smoothing (for instance Lidstone with $k=0.1$) on $B$ otherwise the output will be $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of transition matrix (A): (472, 472)\n",
      "Shape of emission matrix (B): (472, 56057)\n",
      "Shape of initial state probabilities (Pi): (472, 56057)\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist, LidstoneProbDist\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def transition_probs(training_data, tags):\n",
    "    transition_count = np.zeros((len(tags), len(tags)), dtype=np.float64)\n",
    "\n",
    "    for sentence in training_data:\n",
    "        tags_in_sentence = [tag for _, tag in sentence]\n",
    "        bigrams = list(ngrams(tags_in_sentence, 2))\n",
    "        for prev_tag, tag in bigrams:\n",
    "            prev_tag_index = tags.index(prev_tag)\n",
    "            tag_index = tags.index(tag)\n",
    "            transition_count[prev_tag_index][tag_index] += 1\n",
    "\n",
    "    transition_prob = (transition_count.T / transition_count.sum(axis=1)).T\n",
    "\n",
    "    return transition_prob\n",
    "\n",
    "\n",
    "def emission_probs(training_data, tags, words, k):\n",
    "    emission_matrix = np.zeros((len(tags), len(words)), dtype=np.float64)\n",
    "\n",
    "    cfdist = ConditionalFreqDist(\n",
    "        (tag, word) for sentence in training_data for word, tag in sentence\n",
    "    )\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        for j, word in enumerate(words):\n",
    "            freq = cfdist[tag][word]\n",
    "            total = cfdist[tag].N()\n",
    "            smoothed_prob = (freq + k) / (total + k * len(words))\n",
    "            emission_matrix[i][j] = smoothed_prob\n",
    "\n",
    "    return emission_matrix\n",
    "\n",
    "\n",
    "def initial_state_probs(training_data, tags):\n",
    "    initial_state_count = np.zeros(len(tags), dtype=np.float64)\n",
    "    for sentence in training_data:\n",
    "        initial_state_count[tags.index(sentence[0][1])] += 1\n",
    "\n",
    "    initial_state_prob = initial_state_count / initial_state_count.sum()\n",
    "\n",
    "    return initial_state_prob\n",
    "\n",
    "\n",
    "Q = sorted(Q)\n",
    "V = sorted(V)\n",
    "\n",
    "\n",
    "A = transition_probs(training, Q)\n",
    "\n",
    "k = 0.1\n",
    "B = emission_probs(training, Q, V, k)\n",
    "\n",
    "pi = initial_state_probs(training, Q)\n",
    "\n",
    "\n",
    "print(\"Shape of transition matrix (A):\", A.shape)\n",
    "print(\"Shape of emission matrix (B):\", B.shape)\n",
    "print(\"Shape of initial state probabilities (Pi):\", B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a model and we can estimate its performance over the test set.\n",
    "\n",
    "To do this, we need the Viterbi algorithm for the decoding. To help you, an implementation of Viterbi is provided:\n",
    "(note: to use this version you need to assign a numeric id to each word and tag, if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "params is a triple (pi, A, B) where\n",
    "pi = initial probability distribution over states\n",
    "A = transition probability matrix\n",
    "B = emission probability matrix\n",
    "\n",
    "observations is the sequence of observations (in our case, the observed words)\n",
    "\n",
    "the function returns the optimal sequence of states and its score\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def viterbi(params, observations):\n",
    "    pi, A, B = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:, :] = float(\"-inf\")  # cases that have not been treated\n",
    "    backpointers = np.zeros((M, S), \"int\")\n",
    "\n",
    "    # base case\n",
    "    alpha[0, :] = pi * B[:, observations[0]]\n",
    "\n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t - 1, s1] * A[s1, s2] * B[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M - 1, :]))\n",
    "    for i in range(M - 1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "\n",
    "    return list(reversed(ss)), np.max(alpha[M - 1, :])\n",
    "\n",
    "\n",
    "# Example:\n",
    "\n",
    "# original sentence: you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
    "# sentence as sequence of word indexes:\n",
    "# word_index=[42350, 44913, 3024, 50638, 15858, 16209, 36949, 31092, 28334, 45518, 22719, 26179, 32651, 52996, 25205, 16840, 36949, 1402, 46003, 10606, 19795, 3739]\n",
    "\n",
    "# predicted, score = viterbi((pi, A, B), word_index)\n",
    "\n",
    "# predicted will be a sequence of tag indexes:\n",
    "# [12, 55, 86, 39, 29, 4, 70, 7, 14, 7, 0, 6, 21, 28, 12, 55, 28, 27, 28, 0, 9, 14, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentence is:  you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPSS         PPSS\n",
      "      MD*         MD*\n",
      "      QL         QL\n",
      "      RB         RB\n",
      "      VB         VBD\n",
      "      IN         RP\n",
      "      IN         IN\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      VB         VB\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VB         VB\n",
      "      TO         TO\n",
      "      VB         VB\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NP         .\n",
      "      .         .\n",
      "  The sentence is:  Additionally , since you're going to be hors de combat pretty soon with sprue , yaws , Delhi boil , the Granville wilt , liver fluke , bilharziasis , and a host of other complications of the hex you've aroused , you mustn't expect to be lionized socially .\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         RB\n",
      "      ,         ,\n",
      "      CS         CS\n",
      "      PPSS+BER         PPSS+BER\n",
      "      VBG         VBG\n",
      "      TO         TO\n",
      "      BE         BE\n",
      "      FW-RB         VBN\n",
      "      FW-IN         TO\n",
      "      FW-NN         VB\n",
      "      QL         QL\n",
      "      RB         RB\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      NNS         NP\n",
      "      ,         ,\n",
      "      NP         PPSS\n",
      "      NN         VB\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      NP         VBN-TL\n",
      "      NN         NNS-TL\n",
      "      ,         ,\n",
      "      NN         NN\n",
      "      NN         ''\n",
      "      ,         ,\n",
      "      NN         NP\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AP         AP\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         ``\n",
      "      PPSS+HV         PPSS+HV\n",
      "      VBN         VBN\n",
      "      ,         ,\n",
      "      PPSS         PPSS\n",
      "      MD*         MD*\n",
      "      VB         VB\n",
      "      TO         TO\n",
      "      BE         BE\n",
      "      VBN         VBN\n",
      "      RB         RB\n",
      "      .         .\n",
      "  The sentence is:  My advice , if you live long enough to continue your vocation , is that the next time you're attracted by the exotic , pass it up -- it's nothing but a headache .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VB         VB\n",
      "      JJ         JJ\n",
      "      QLP         QLP\n",
      "      TO         TO\n",
      "      VB         VB\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      BEZ         BEZ\n",
      "      CS         CS\n",
      "      AT         AT\n",
      "      AP         AP\n",
      "      NN         NN\n",
      "      PPSS+BER         PPSS+BER\n",
      "      VBN         VBN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      ,         ,\n",
      "      VB         VB\n",
      "      PPO         PPO\n",
      "      RP         RP\n",
      "      --         --\n",
      "      PPS+BEZ         PPS+BEZ\n",
      "      PN         PN\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      .         .\n",
      "  The sentence is:  As you can count on me to do the same .\n",
      "   ##TRUE##    ##PRED##\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      MD         MD\n",
      "      VB         VB\n",
      "      IN         IN\n",
      "      PPO         PPO\n",
      "      TO         TO\n",
      "      DO         DO\n",
      "      AT         AT\n",
      "      AP         AP\n",
      "      .         .\n",
      "  The sentence is:  Compassionately yours ,\n",
      "   ##TRUE##    ##PRED##\n",
      "      RB         ``\n",
      "      PP$$         UH\n",
      "      ,         ,\n",
      "  The sentence is:  S. J. Perelman\n",
      "   ##TRUE##    ##PRED##\n",
      "      NP         NP\n",
      "      NP         NP\n",
      "      NP         NP\n",
      "  The sentence is:  revulsion in the desert\n",
      "   ##TRUE##    ##PRED##\n",
      "      NN-HL         NN\n",
      "      IN-HL         IN\n",
      "      AT-HL         AT\n",
      "      NN-HL         NN\n",
      "  The sentence is:  the doors of the D train slid shut , and as I dropped into a seat and , exhaling , looked up across the aisle , the whole aviary in my head burst into song .\n",
      "   ##TRUE##    ##PRED##\n",
      "      AT         AT\n",
      "      NNS         NNS\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NP-TL         NN\n",
      "      NN         NN\n",
      "      VBD         VBD\n",
      "      VBN         VBN\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      CS         CS\n",
      "      PPSS         PPSS\n",
      "      VBD         VBD\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      ,         ,\n",
      "      VBG         NP\n",
      "      ,         ,\n",
      "      VBD         VBD\n",
      "      RP         RP\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NN         NNS\n",
      "      IN         IN\n",
      "      PP$         PP$\n",
      "      NN         NN\n",
      "      VBD         NN\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      .         .\n",
      "  The sentence is:  She was a living doll and no mistake -- the blue-black bang , the wide cheekbones , olive-flushed , that betrayed the Cherokee strain in her Midwestern lineage , and the mouth whose only fault , in the novelist's carping phrase , was that the lower lip was a trifle too voluptuous .\n",
      "   ##TRUE##    ##PRED##\n",
      "      PPS         PPS\n",
      "      BEDZ         BEDZ\n",
      "      AT         AT\n",
      "      VBG         VBG\n",
      "      NN         NN\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      --         --\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      NNS         NNS\n",
      "      ,         ,\n",
      "      JJ         NP\n",
      "      ,         ,\n",
      "      WPS         WPS\n",
      "      VBD         VBD\n",
      "      AT         AT\n",
      "      NP         JJ\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      PP$         PP$\n",
      "      JJ-TL         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      CC         CC\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      WP$         WP$\n",
      "      AP         AP\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      NN$         NN$\n",
      "      VBG         NN\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      BEDZ         BEDZ\n",
      "      CS         CS\n",
      "      AT         AT\n",
      "      JJR         JJR\n",
      "      NN         NN\n",
      "      BEDZ         BEDZ\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      QL         QL\n",
      "      JJ         JJ\n",
      "      .         .\n",
      "  The sentence is:  From what I was able to gauge in a swift , greedy glance , the figure inside the coral-colored boucle dress was stupefying .\n",
      "   ##TRUE##    ##PRED##\n",
      "      IN         IN\n",
      "      WDT         WDT\n",
      "      PPSS         PPSS\n",
      "      BEDZ         BEDZ\n",
      "      JJ         JJ\n",
      "      IN         IN\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         JJ\n",
      "      ,         ,\n",
      "      JJ         JJ\n",
      "      NN         NN\n",
      "      ,         ,\n",
      "      AT         AT\n",
      "      NN         NN\n",
      "      IN         IN\n",
      "      AT         AT\n",
      "      JJ         QL\n",
      "      NN         JJ\n",
      "      NN         NN\n",
      "      BEDZ         BEDZ\n",
      "      VBG         VBN\n",
      "      .         .\n"
     ]
    }
   ],
   "source": [
    "# Example of results calculation\n",
    "word_to_index = {word: i for i, word in enumerate(V)}\n",
    "\n",
    "testing_formated = []\n",
    "for sentence in testing:\n",
    "    sent = \"\"\n",
    "    words_index = []  # vector of word indices to be passed to Viterbi\n",
    "    true_label = []  # vector of the true labels from labeled corpus\n",
    "    for word, tag in sentence:\n",
    "        words_index.append(\n",
    "            word_to_index[word]\n",
    "        )  # word_to_index is a dictionary mapping a word to its index\n",
    "        true_label.append(tag)\n",
    "        sent = sent + \" \" + word\n",
    "    testing_formated.append((words_index, true_label, sent))\n",
    "\n",
    "\n",
    "for word_index, labels, sentence in testing_formated:\n",
    "    print(\"  The sentence is:\", sentence)\n",
    "    print(\"   ##TRUE##    ##PRED##\")\n",
    "    predicted, score = viterbi((pi, A, B), word_index)  # call the viterbi decoder\n",
    "    for i, true_label in enumerate(labels):\n",
    "        predicted_label = Q[\n",
    "            predicted[i]\n",
    "        ]  # Q here is the vector of tags, so that at Q[i] we have the i_th tag in literal form\n",
    "        print(\"      \" + true_label + \"         \" + predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: calculate Precision, Recall and F-measure for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.47280334728033474\n",
      "Recall: 0.47280334728033474\n",
      "F-measure: 0.47280334728033474\n"
     ]
    }
   ],
   "source": [
    "# Function to perform part-of-speech tagging using the bigram model\n",
    "def pos_tagging(\n",
    "    sentence,\n",
    "    transition_matrix,\n",
    "    emission_matrix,\n",
    "    initial_state_probabilities,\n",
    "    tags,\n",
    "    words,\n",
    "):\n",
    "    predicted_tags = []\n",
    "    prev_tag = None\n",
    "    for word in sentence:\n",
    "        if word.lower() in words:\n",
    "            word_index = words.index(word.lower())\n",
    "        else:\n",
    "            word_index = 0\n",
    "        if prev_tag:\n",
    "            prev_tag_index = tags.index(prev_tag)\n",
    "            transition_probs = transition_matrix[prev_tag_index]\n",
    "            emission_prob = emission_matrix[:, word_index]\n",
    "            joint_probs = transition_probs * emission_prob\n",
    "\n",
    "            predicted_tag_index = np.argmax(joint_probs)\n",
    "            predicted_tag = tags[predicted_tag_index]\n",
    "            predicted_tags.append(predicted_tag)\n",
    "        else:\n",
    "            predicted_tag_index = np.argmax(initial_state_probabilities)\n",
    "            predicted_tag = tags[predicted_tag_index]\n",
    "            predicted_tags.append(predicted_tag)\n",
    "        prev_tag = predicted_tag\n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    test_data,\n",
    "    transition_matrix,\n",
    "    emission_matrix,\n",
    "    initial_state_probabilities,\n",
    "    tags,\n",
    "    words,\n",
    "):\n",
    "    true_positive, false_positive, false_negative = 0, 0, 0\n",
    "\n",
    "    for sentence in test_data:\n",
    "        words_in_sentence = [word.lower() for word, _ in sentence]\n",
    "        gold_tags = [tag for _, tag in sentence]\n",
    "        predicted_tags = pos_tagging(\n",
    "            words_in_sentence,\n",
    "            transition_matrix,\n",
    "            emission_matrix,\n",
    "            initial_state_probabilities,\n",
    "            tags,\n",
    "            words,\n",
    "        )\n",
    "        for gold_tag, predicted_tag in zip(gold_tags, predicted_tags):\n",
    "            if gold_tag == predicted_tag:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                if gold_tag in tags:\n",
    "                    false_negative += 1\n",
    "                if predicted_tag in tags:\n",
    "                    false_positive += 1\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    f_measure = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f_measure\n",
    "\n",
    "\n",
    "precision, recall, f_measure = evaluate_model(testing, A, B, pi, Q, V)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: modify your HMM to use trigrams instead of bigrams, and re-evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5690376569037657\n",
      "Recall: 0.5690376569037657\n",
      "F-measure: 0.5690376569037657\n"
     ]
    }
   ],
   "source": [
    "def transition_probs_trigram(training_data, tags):\n",
    "    trigram_transition_count = np.zeros(\n",
    "        (len(tags), len(tags), len(tags)), dtype=np.float64\n",
    "    )\n",
    "\n",
    "    for sentence in training_data:\n",
    "        tags_in_sentence = [tag for _, tag in sentence]\n",
    "        trigrams = list(ngrams(tags_in_sentence, 3))\n",
    "        for prev_prev_tag, prev_tag, tag in trigrams:\n",
    "            prev_prev_tag_index = tags.index(prev_prev_tag)\n",
    "            prev_tag_index = tags.index(prev_tag)\n",
    "            tag_index = tags.index(tag)\n",
    "            trigram_transition_count[prev_prev_tag_index][prev_tag_index][\n",
    "                tag_index\n",
    "            ] += 1\n",
    "\n",
    "    trigram_transition_prob = np.zeros(\n",
    "        (len(tags), len(tags), len(tags)), dtype=np.float64\n",
    "    )\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(tags)):\n",
    "            total_count = np.sum(trigram_transition_count[i, j])\n",
    "            if total_count > 0:\n",
    "                trigram_transition_prob[i, j] = (\n",
    "                    trigram_transition_count[i, j] / total_count\n",
    "                )\n",
    "\n",
    "    return trigram_transition_prob\n",
    "\n",
    "\n",
    "def pos_tagging(\n",
    "    sentence,\n",
    "    transition_matrix,\n",
    "    emission_matrix,\n",
    "    initial_state_probabilities,\n",
    "    tags,\n",
    "    words,\n",
    "):\n",
    "    predicted_tags = []\n",
    "    prev_prev_tag = None\n",
    "    prev_tag = None\n",
    "    for word in sentence:\n",
    "        if word.lower() in words:\n",
    "            word_index = words.index(word.lower())\n",
    "        else:\n",
    "            word_index = 0\n",
    "        if prev_tag and prev_prev_tag:\n",
    "            prev_prev_tag_index = tags.index(prev_prev_tag)\n",
    "            prev_tag_index = tags.index(prev_tag)\n",
    "            transition_probs = transition_matrix[prev_prev_tag_index, prev_tag_index]\n",
    "            emission_prob = emission_matrix[:, word_index]\n",
    "            joint_probs = transition_probs * emission_prob\n",
    "\n",
    "            predicted_tag_index = np.argmax(joint_probs)\n",
    "            predicted_tag = tags[predicted_tag_index]\n",
    "            predicted_tags.append(predicted_tag)\n",
    "        else:\n",
    "            predicted_tag_index = np.argmax(initial_state_probabilities)\n",
    "            predicted_tag = tags[predicted_tag_index]\n",
    "            predicted_tags.append(predicted_tag)\n",
    "        prev_prev_tag = prev_tag\n",
    "        prev_tag = predicted_tag\n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "precision, recall, f_measure = evaluate_model(\n",
    "    testing, transition_probs_trigram(training, Q), B, pi, Q, V\n",
    ")\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK's HMM implementation\n",
    "\n",
    "We will compare now our model built from scratch to the implementation provided by NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VB'), ('up', 'IN'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', 'NP'), ('.', '.')]\n",
      "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VBD'), ('up', 'RP'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', '.'), ('.', '.')]\n",
      "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'FW-RB'), ('de', 'FW-IN'), ('combat', 'FW-NN'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NNS'), (',', ','), ('Delhi', 'NP'), ('boil', 'NN'), (',', ','), ('the', 'AT'), ('Granville', 'NP'), ('wilt', 'NN'), (',', ','), ('liver', 'NN'), ('fluke', 'NN'), (',', ','), ('bilharziasis', 'NN'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', 'NN'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
      "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'VBN'), ('de', 'TO'), ('combat', 'VB'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NP'), (',', ','), ('Delhi', 'PPSS'), ('boil', 'VB'), (',', ','), ('the', 'AT'), ('Granville', 'VBN-TL'), ('wilt', 'NNS-TL'), (',', ','), ('liver', 'NN'), ('fluke', \"''\"), (',', ','), ('bilharziasis', 'NP'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', '``'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
      "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'PPSS+BER'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
      "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'PPSS+BER'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
      "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
      "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
      "[('Compassionately', 'RB'), ('yours', 'PP$$'), (',', ',')]\n",
      "[('Compassionately', '``'), ('yours', 'UH'), (',', ',')]\n",
      "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
      "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
      "[('revulsion', 'NN-HL'), ('in', 'IN-HL'), ('the', 'AT-HL'), ('desert', 'NN-HL')]\n",
      "[('revulsion', 'NN'), ('in', 'IN'), ('the', 'AT'), ('desert', 'NN')]\n",
      "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NP-TL'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'VBG'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NN'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'VBD'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
      "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NN'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'NP'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NNS'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'NN'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
      "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'JJ'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'NP'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ-TL'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'VBG'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
      "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'NP'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'JJ'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'NN'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
      "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'JJ'), ('boucle', 'NN'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBG'), ('.', '.')]\n",
      "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'QL'), ('boucle', 'JJ'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer(states=Q, symbols=V)\n",
    "\n",
    "model = trainer.train_supervised(\n",
    "    training, estimator=lambda fd, bins: hmm.LidstoneProbDist(fd, 0.1, bins)\n",
    ")\n",
    "\n",
    "for sent in testing:\n",
    "    u_sent = []\n",
    "    for word, tag in sent:\n",
    "        u_sent.append(word)\n",
    "    tagged = model.tag(u_sent)\n",
    "    print(sent)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Calculate precision, recall and F-measure and compare them to the results that you obtained with the two models (bigram and trigram) that you implemented before. Can you deduce whether the NLTK model is using bigrams or trigrams? (It is not stated in the manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8888888888888888\n",
      "Recall: 0.8163265306122449\n",
      "F-measure: 0.8510638297872342\n"
     ]
    }
   ],
   "source": [
    "true_tags = []\n",
    "predicted_tags = []\n",
    "\n",
    "for sent in testing:\n",
    "    words = [word for word, tag in sent]\n",
    "    true_tags.extend([tag for word, tag in sent])\n",
    "    predicted_tags.extend([tag for word, tag in model.tag(words)])\n",
    "\n",
    "true_tags_set = set(true_tags)\n",
    "predicted_tags_set = set(predicted_tags)\n",
    "\n",
    "precision = nltk.precision(true_tags_set, predicted_tags_set)\n",
    "recall = nltk.recall(true_tags_set, predicted_tags_set)\n",
    "f_measure = nltk.f_measure(true_tags_set, predicted_tags_set)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Conditional Random Fields\n",
    "\n",
    "For this exercise we will need to use the sklearn_crfsuite package. If it is not installed, it can be installed using pip with ```pip install sklearn-crfsuite```.\n",
    "\n",
    "We will work on a Kaggle dataset named ```ner_dataset.csv``` (it should be in the same directory as the notebook).\n",
    "\n",
    "Pandas can be used to read the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/_0ss5l5d6nz8d575gs5spbx40000gn/T/ipykernel_51829/3186707956.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.fillna(method=\"ffill\") #repeat sentence number on each row\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fresh-cut', 'Thirty-six', 'surroundings', '130.6', 'voyage', 'Coup', 'viewership', 'Holder', 'horrific', 'Group-E']\n",
      "35177\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")  # repeat sentence number on each row\n",
    "\n",
    "words = list(set(data[\"Word\"].values))  # vocabulary V\n",
    "n_words = len(words)\n",
    "\n",
    "print(words[:10])\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with some code that can read the sentences and produce the features in the format required by crf_suite. The ```SentenceGetter``` class transforms sentences into sequences of ```(word, POS, tag)``` triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/_0ss5l5d6nz8d575gs5spbx40000gn/T/ipykernel_51829/985691969.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [\n",
    "            (w, p, t)\n",
    "            for w, p, t in zip(\n",
    "                s[\"Word\"].values.tolist(),\n",
    "                s[\"POS\"].values.tolist(),\n",
    "                s[\"Tag\"].values.tolist(),\n",
    "            )\n",
    "        ]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "# load data\n",
    "getter = SentenceGetter(data)  # transform sentences into sequences of (Word, POS, Tag)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function allows us to define features that are used in the CRF. The features are stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    input:\n",
    "       sent: sentence in the format of sequence of (Word, POS, Tag) triples\n",
    "       i: position in the sentence\n",
    "    output:\n",
    "       features: a dictionary mapping the feature name into a value\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {  # features related to the current position\n",
    "        \"bias\": 1.0,\n",
    "        \"word.lower()\": word.lower(),\n",
    "        \"postag\": postag,\n",
    "    }\n",
    "    if i > 0:  # features related to preceding word/tag\n",
    "        word1 = sent[i - 1][0]\n",
    "        postag1 = sent[i - 1][1]\n",
    "        features.update(\n",
    "            {\n",
    "                \"-1:word.lower()\": word1.lower(),\n",
    "                \"-1:postag\": postag1,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        features[\"BOS\"] = True  # feature for Beginning of Sentence\n",
    "\n",
    "    if i < len(sent) - 1:  # features related to the following word/tag\n",
    "        word1 = sent[i + 1][0]\n",
    "        postag1 = sent[i + 1][1]\n",
    "        features.update(\n",
    "            {\n",
    "                \"+1:word.lower()\": word1.lower(),\n",
    "                \"+1:postag\": postag1,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        features[\"EOS\"] = True  # feature for end of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    # transforms the sentence in a sequence of features\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    # transforms the sentence in a sequence of labels\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    # transforms the sentence in a sequence of tokens (removes POS tags and labels)\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the features and label vectors, and create a CRF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn_crfsuite\n",
      "  Obtaining dependency information for sklearn_crfsuite from https://files.pythonhosted.org/packages/b2/11/a8370dd6fce65f8f4e74a0adffae72be9db5799d8ed8ddbf84415356a764/sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite)\n",
      "  Obtaining dependency information for python-crfsuite>=0.9.7 from https://files.pythonhosted.org/packages/bc/32/743048adf41ba3ebc4d82deed5d4a336164dc0066ef83b28d2a4b1979d66/python_crfsuite-0.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading python_crfsuite-0.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /Users/maximemoutet/.pyenv/versions/3.11.5/envs/venv-nlp/lib/python3.11/site-packages (from sklearn_crfsuite) (1.3.2)\n",
      "Collecting tabulate>=0.4.2 (from sklearn_crfsuite)\n",
      "  Obtaining dependency information for tabulate>=0.4.2 from https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/maximemoutet/.pyenv/versions/3.11.5/envs/venv-nlp/lib/python3.11/site-packages (from sklearn_crfsuite) (4.66.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/maximemoutet/.pyenv/versions/3.11.5/envs/venv-nlp/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/maximemoutet/.pyenv/versions/3.11.5/envs/venv-nlp/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/maximemoutet/.pyenv/versions/3.11.5/envs/venv-nlp/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/maximemoutet/.pyenv/versions/3.11.5/envs/venv-nlp/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.2.0)\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp311-cp311-macosx_11_0_arm64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate, python-crfsuite, sklearn_crfsuite\n",
      "Successfully installed python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0 tabulate-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(algorithm=\"lbfgs\", max_iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a model with gradient descent algorithm (\"lbfgs\") and a limit of $100$ iterations.\n",
    "\n",
    "Now we build the model and evaluate it on a 66/33 split between training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.40      0.01      0.03       137\n",
      "       B-eve       0.69      0.31      0.43       111\n",
      "       B-geo       0.83      0.91      0.87     12357\n",
      "       B-gpe       0.98      0.82      0.89      5226\n",
      "       B-nat       0.60      0.09      0.15        69\n",
      "       B-org       0.78      0.67      0.72      6762\n",
      "       B-per       0.83      0.79      0.81      5649\n",
      "       B-tim       0.93      0.84      0.88      6650\n",
      "       I-art       0.38      0.02      0.05       124\n",
      "       I-eve       0.56      0.21      0.31        89\n",
      "       I-geo       0.80      0.79      0.80      2433\n",
      "       I-gpe       0.95      0.33      0.49        55\n",
      "       I-nat       0.33      0.05      0.08        21\n",
      "       I-org       0.76      0.78      0.77      5545\n",
      "       I-per       0.85      0.87      0.86      5730\n",
      "       I-tim       0.81      0.74      0.78      2110\n",
      "           O       0.99      0.99      0.99    292571\n",
      "\n",
      "    accuracy                           0.97    345639\n",
      "   macro avg       0.73      0.54      0.58    345639\n",
      "weighted avg       0.97      0.97      0.96    345639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "pred = crf.predict(X_test)\n",
    "\n",
    "n_pred = [item for sublist in pred for item in sublist]\n",
    "n_test = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "report = classification_report(y_pred=n_pred, y_true=n_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report shows accuracy stats for all classes, but we are not interested in the **O** class. We can see that the scores for people names, place names and organizations are relatively low.\n",
    "\n",
    "**Exercise 6**: Can you think of some new features for the CRF model to improve the results, especially on **B-org** ? Modify the *word2features* function to include the additional features and compare with the above results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all we need to retrieve the number of different tags (a.k.a categories or classes of the words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags)\n",
    "\n",
    "vocab = {w: i + 1 for i, w in enumerate(words)}  # map words into a number\n",
    "tag_map = {t: i for i, t in enumerate(tags)}  # map tags into a number\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 75\n",
    "\n",
    "X = [[vocab[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(\n",
    "    maxlen=max_len, sequences=X, padding=\"post\", value=n_words\n",
    ")  # pad with special token PAD, with ID=n_words\n",
    "y = [[tag_map[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(\n",
    "    maxlen=max_len, sequences=y, padding=\"post\", value=-1\n",
    ")  # -1 is associated to the PAD token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER using a LSTM model\n",
    "\n",
    "In this final section we will see an example of a neural network model written in PyTorch that uses a LSTM-based architecture for Named Entity Recognition.\n",
    "\n",
    "First of all, we will prepare the data to have all information coded numerically (words and tags) and the sentences padded to a max length, in order to have all sentences of the same size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all we need to retrieve the number of different tags (a.k.a categories or classes of the words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags)\n",
    "\n",
    "vocab = {w: i + 1 for i, w in enumerate(words)}  # map words into a number\n",
    "tag_map = {t: i for i, t in enumerate(tags)}  # map tags into a number\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 75\n",
    "\n",
    "X = [[vocab[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(\n",
    "    maxlen=max_len, sequences=X, padding=\"post\", value=n_words\n",
    ")  # pad with special token PAD, with ID=n_words\n",
    "y = [[tag_map[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(\n",
    "    maxlen=max_len, sequences=y, padding=\"post\", value=-1\n",
    ")  # -1 is associated to the PAD token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the data and split them into training and test. We set batch size at 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from NERDataset import NERDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "ner_train = NERDataset(X_train, y_train)\n",
    "ner_test = NERDataset(X_test, y_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(ner_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model is defined here. We have an embedding that maps each word in a vector (embedding) of size 100, which is learnt from the dataset. The embedded sentence is fed to a LSTM layer of size 50. The output is transferred to a fully connected layer with *n_tags* output, one for each of the possible labels. The loss is a cross-entropy loss over all tokens (excluding the \"pad\" tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vocab_size = n_words + 1\n",
    "embedding_dim = 100\n",
    "lstm_hidden_dim = 50\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # maps each token to an embedding_dim vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        # fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, n_tags)\n",
    "\n",
    "    def forward(self, s):\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)  # dim: batch_size x batch_max_len x embedding_dim\n",
    "\n",
    "        # run the LSTM along the sentences of length batch_max_len. We discard the cell state as output\n",
    "        s, _ = self.lstm(s)  # dim: batch_size x batch_max_len x lstm_hidden_dim\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "\n",
    "        # apply the fully connected layer and obtain the output for each token\n",
    "        s = self.fc(s)  # dim: batch_size*batch_max_len x num_tags\n",
    "\n",
    "        return F.log_softmax(s, dim=1)  # dim: batch_size*batch_max_len x num_tags\n",
    "\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # discard 'PAD' tokens\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # the number of tokens is the sum of elements in mask\n",
    "    num_tokens = int(torch.sum(mask))\n",
    "\n",
    "    # pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), labels] * mask\n",
    "\n",
    "    # cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs) / num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block we carry out the training over 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximemoutet/.pyenv/versions/3.11.5/envs/venv-nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 1.667\n",
      "Loss after mini-batch  1000: 0.838\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.753\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.635\n",
      "Loss after mini-batch  1000: 0.576\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.534\n",
      "Loss after mini-batch  1000: 0.495\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.460\n",
      "Loss after mini-batch  1000: 0.426\n"
     ]
    }
   ],
   "source": [
    "network = Net()\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "# Run the training loop for defined number of epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "    # Print epoch\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    i = 0\n",
    "    for inputs, targets in trainloader:\n",
    "        # print(inputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = network(inputs)  # Perform forward pass\n",
    "        loss = loss_fn(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backprop\n",
    "        optimizer.step()  # Optimization\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print(\"Loss after mini-batch %5d: %.3f\" % (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
